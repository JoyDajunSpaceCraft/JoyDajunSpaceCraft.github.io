---
layout: post
title: ' Pytorch '
subtitle: ' pytorch '
date: 2021--01-06
author: 'Joy'
header-img: 'img/pytorch/title.PNG'
tags:
  - Python
  - Pytorch
  - Numpy
  - self-learning
---

Pytorch Totural <a href="https://www.youtube.com/playlist?list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4" target="_blank">YouTube网址</a>

# 创建虚拟环境
`conda create -n pytorch python=3.7`

表示用conda创建了一个虚拟的Python3.7环境，名称为pytorch

`conda activate pytorch` 就在这个pytorch虚拟环境中，在pytorch官网上下载的命令`conda install pytorch torchvision -c pytorch`帮助下载所有需要的库
# tensor base 基础
重要的数据库，可认为是一个高维数组

```
import torch 
x = torch.empty(2,3)
```

+ 设置数据的格式 
常见的有 float16`x = torch.ones(2,2,dtype=torch.int)`。可以看大小`x.size()`。

+ 创建一个Tensor `x = torch.tensor(维度)`或者`x = torch.rand(维度)`也能显示维度。

+ 加减乘除
`z = x+y`或者`z = torch.add(x, y)`或者`y.add_(x)`也能实现相加，注意`add_`方法有一种替换的意味表示`+=`

```
z = y-x
z = torch.sub(y,x)

z = y*x
z = torch.mul(x,y)
y.mul_(x)

z = y/x
z =  torch.div(y,x)
```

+ 切片

```
x = torch.rand(5,3)
x[:,0]
x[1,:]
x[1,1].item() # 获得的是这个Tensor的值，只能用在只有一个element的tensor上
```

+ reshape

```
x = torch.rand(4,4)
y = x.view(-1,8)
y.size() # 是
```

+ numpy 和 tensor转换

```
a = torch.ones(1,1)
b = a.numpy()
print(type(b)) # 查看b的类型
```

**在GPU而不是CPU的时候所有的numpy.array 共享同一块地址如果a.add_(1)同时b的值也会有变化**

```
a = np.ones(5)
b = torch.from_numpy(a) # numpy -> tensor
```

**同样也会因为CPU GPU而导致存储在同一个地址中的问题**

## 解决 GPU运行时存储同一个地址的问题

```
if torch.cuda.is_available():
  device = torch.device("cuda")
  x = torch.ones(5,device=device)
  y = torch.ones(5)
  y = y.to(device)
  z = x + y
  z = z.to("cpu")

```

**注意GPU不允许使用numpy所以不能将z直接变为numpy的格式，所以要`z = z.to("cpu")`再转换为numpy**


+ 是否grad 

`x = torch.tensor(5, requires_grad=True)`表示这个tensor在日后需要求梯度,requires_grad默认是False, 设为True后会创建 computation graph

# autograd 自动求导

```
x = torch.rand(3, requires_grad=True) # 创建 computation graph ,tensor([0.5864, 0.0808, 0.6712], requires_grad=True)

y = x+2 # tensor([2.5864, 2.0808, 2.6712], grad_fn=<AddBackward0>)

z = y*y*2
z = z.mean() # tensor(12.1033, grad_fn=<MeanBackward0>)
z.backward() # dz/dx
x.grad()
```
![](/img/pytorch/autograd.jpg)

根据 chain rules 需要引入一个Jacobian变量对内容进行实现，具体就是在求导时需要一个3*1向量对原始的3*3向量进行变换
![](/img/pytorch/v.jpg)

相对应的如果没有进行 mean方法最后对应的代码就不能进行求导

```
x = torch.rand(3, requires_grad=True)
y = x*2
z = y*y*2
v = torch.tensor([0.1, 1.0, 0.001], dtype=float32)
z.backward(v)
```

## 怎样设置不需要autograd的tensor
1. x.requires_grad(False)

2. x.detach() 前提是创建了一个新的Tensor
 
3. with torch.no_grad():

## 如何正确使用
梯度在反向传播之前是累加的，每次运算的时候都会累加之前的梯度，所以反向传播之前需要将梯度清零

```
weight = torch.ones(3, requires_grad=True)

for epoch in range(2):
  model_output = (weight * 3).sum()
  model_output.backward()
  weight.grad # 每次梯度都会累加
  weight.grad.zero_()
```

## 在optimizer上清除梯度

```
optimizer = torch.optim.SGD(weight, lr=0.01)
optimizer.step()
optimizer.zero_grad()
```

# 原理讲解 backpropagation Theory and Examples
![](/img/pytorch/chainRule.jpg)
链式法则
![](/imh/pytorch/gradGraph.jpg)
梯度图：因为最后的loss已知，要求dloss/dx

包含三部
1. Forward Pass:Compute Loss
2. Compute Local Gradient 
3. Backward pass dLoss / dWeights 

![](/img/pytorch/graphDetail.jpg)

```
x = torch.tensor(1.0)
y = torch.tensor(2.0)

w = torch.tensor(1.0, requires_grad=True)

# forward pass and compute the loss
y_hat = w * x
loss = (y_hat - y)**2

# backward pass
loss.backward() # w.grad = 2

# update weight

# next forward and backward 
```

# gradient decent 
1. predict 手动
2. gradient computation: autograd 
3. loss computation: pytorch loss
4. paramter update: pytorch optimization
这一节先完成 1，2

+ 利用numpy完成

```
import numpy as np
# f = w * x
# f = 2 * x
X = np.array([1,2,3,4], dtype=np.float32)
Y = np.array([2,4,6,8], dtype=np.float32)

w = 0.0

# modal prediction
def forward(x):
  return w * x

# loss = MSE 
def loss(y, y_predict):
  return ((y_predict - y)**2).mean()

# gradient
# MSE = 1/N * (w*x - y) ** 2
# dJ/dw = 1/N * 2x * (wx - y)
def gradient(x, y, y_predict):
  return np.dot(2*x, y_predict-y).mean()

# training
learning_rate = 0.01
n_iters = 10
for epoch in range(n_iters):
  # prediction = forward pass
  y_pred = forward(X)
  
  # loss
  l = loss(Y, y_pred)
   
  # gradient
  dw = gradient(X,Y,y_predict)

  # update weight 
  w -= learning_rate * dw

  if epoch%1==0:
    print()

```

+ 利用pytorch完成

```
import torch
# f = w * x
# f = 2 * x
X = torch.tensor([1,2,3,4], dtype=torch.float32)
Y = torch.tensor([2,4,6,8], dtype=torch.float32)

w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)

# modal prediction
def forward(x):
  return w * x

# loss = MSE 
def loss(y, y_predict):
  return ((y_predict - y)**2).mean()


# training
learning_rate = 0.01
n_iters = 10

for epoch in range(n_iters):
  # prediction = forward pass
  y_pred = forward(X)
  
  # loss
  l = loss(Y, y_pred)
   
  # gradient = backward pass 
  l.backward() # dl/dw

  # update weight 不需要加到运算图里面
  with torch.no_grad():
    w -= learning_rate * w.grad()

  # zero grad 
  w.grad.zero_()
  if epoch%1==0:
  print()
```

最后的结果不是可计算的，因为pytorch中不是numerical计算的，增加计算epoch可解决

# Training pipline
1. Design model (input, output size, forward pass)
2. Construct loss and optimizer
3. Training loop

    - forward pass: prediction
    - backward pass: gradient
    - update weight


以上的流程pytorch都能帮着做,pytorch明白需要优化的是w所以不需要再定义w`model = nn.Linear(input_size, output_size)`

```
import torch 
import torch.nn as nn

# 为了让pytorch明白这个状态，将XY变成2D的array 
X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)
Y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)

# 测试集 表示将这个值放到计算图里面 通过结果判断计算图拟合是否成功
X_test = torch.tensor([5], dtype=torch.float32)

n_samples, n_features = X.shape # (4,1)

input_size = n_features
output_size = n_features 

# w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)
# modal prediction
# def forward(x):
#   return w * x
model = nn.Linear(input_size, output_size)

# 自定义 LinearRgression 
class LinearRegression(nn.Module):

  def __init__(self, input_dim, output_dim):
    super(LinearRegress, self).__init__()
    # define layers
    self.lin = nn.Linear(input_dim, output_dim)
  
  def forward(self, x):
    self.lin(x)

model = LinearRegression(input_size, output_size)



# loss = MSE 
# def loss(y, y_predict):
#   return ((y_predict - y)**2).mean()
loss = nn.MSELoss()

# training
learning_rate = 0.01
n_iters = 10

optimizer = nn.torch.SGD(modal.paramters(), lr=learning_rate)

for epoch in range(n_iters):
  # prediction = forward pass
  # y_pred = forward(X)
  y_pred = model(X)
  
  # loss
  l = loss(Y, y_pred) # 不需要改变参数因为loss是callable函数
   
  # gradient = backward pass 
  l.backward() # dl/dw

  # update weight 
  # with torch.no_grad():
  #   w -= learning_rate * w.grad()
  optimizer.step()
  
  # zero grad 
  # w.grad.zero_()
  optimizer.zero_grad()

  if epoch%10==0:
    [w,b] = model.paramters()# 打印超参数
```

# Linear Regression 

还是按照原先的三步
1. Design model (input, output size, forward pass)
2. Construct loss and optimizer
3. Training loop

    - forward pass: prediction
    - backward pass: gradient
    - update weight

```
import torch 
import torch.nn as nn
import numpy as np
import sklearn import datasets
import matplotlib.pyplot as plt

# 0. prepare data
X_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1)

X = torch.from_numpy(X_numpy.astype(np.float32))
y = torch.from_numpy(y_numpy.astype(np.float32))

y = y.view(y.shape[0],1)

n_sample, n_features = X.shape

input_size = n_sample
output_size = 1
# 1. model
model = nn.Linear(input_size, output_size)

# 2. loss and optimizer
learning_rate = 0.01
criturion = nn.MSELoss()
optimizer = torch.optim.SGD(model.paramters(), lr=learning_rate)

# 3. training loop
num_epochs = 100
for epoch in range(num_epochs):
  # forward pass and loss
  y_predict = model(X)
  loss = criterion(y_predict,y)

  # backward pass
  loss.backward()

  # update 
  optimizer.step()

  optimizer.zero_grad()


# plot
preidct = model(X).detach().numpy()# tensor 从计算图中剥离，并转换为numpy格式
plt.plot(X_numpy, y_numpy, 'ro') #首先画出初始值，用red dot ro

plt.plot(X_numpy, predict,'b')# 打印出预测值 用blue

plt.show()

```


# Logistic Regression
1. Design model (input, output size, forward pass)
2. Construct loss and optimizer
3. Training loop
    - forward pass: prediction
    - backward pass: gradient
    - update weight
相对于linear regression 修改了 loss和基本的model

```
import torch 
import torch.nn as nn
import numpy as np
from sklearn import datasets
from sklearn.preprocessing import StandardScaler


```

papapap